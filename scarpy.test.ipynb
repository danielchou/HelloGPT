{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 89\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# 運行爬蟲\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m \u001b[43mexecute_spider\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# 將結果保存為 JSON 文件\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menhanced_output.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[14], line 86\u001b[0m, in \u001b[0;36mexecute_spider\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m     85\u001b[0m     future \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39msubmit(run_spider)\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\crochet\\_eventloop.py:449\u001b[0m, in \u001b[0;36mEventLoop.wait_for.<locals>.decorator.<locals>.wrapper\u001b[1;34m(function, _, args, kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m eventual_result \u001b[38;5;241m=\u001b[39m run()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meventual_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m:\n\u001b[0;32m    451\u001b[0m     eventual_result\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\crochet\\_eventloop.py:194\u001b[0m, in \u001b[0;36mEventualResult.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threadable\u001b[38;5;241m.\u001b[39misInIOThread():\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEventualResult.wait() must not be run in the reactor thread.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 194\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Failure):\n\u001b[0;32m    196\u001b[0m     result\u001b[38;5;241m.\u001b[39mraiseException()\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\crochet\\_eventloop.py:173\u001b[0m, in \u001b[0;36mEventualResult._result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# In Python 2.6 we can't rely on the return result of wait(), so we\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# have to check manually:\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_set\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_retrieved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[1;31mTimeoutError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from scrapy.utils.log import configure_logging\n",
    "from twisted.internet import reactor\n",
    "import json\n",
    "from crochet import setup, wait_for\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# 初始化 crochet\n",
    "setup()\n",
    "\n",
    "class EnhancedSpider(scrapy.Spider):\n",
    "    name = 'enhanced_spider'\n",
    "    start_urls = ['https://www.esunbank.com/zh-tw/personal/credit-card/discount/shops']  # 替換為您的目標網站URL\n",
    "\n",
    "    def parse(self, response):\n",
    "        # 提取當前頁面的所有可讀文本\n",
    "        all_text = ' '.join(response.xpath('//body//text()').getall()).strip()\n",
    "        \n",
    "        # 提取當前頁面的標題\n",
    "        title = response.css('title::text').get()\n",
    "        \n",
    "        # 提取分頁信息\n",
    "        pagination = {\n",
    "            'first': response.css('.first::attr(href)').get(),\n",
    "            'prev': response.css('.prev::attr(href)').get(),\n",
    "            'next': response.css('.next::attr(href)').get(),\n",
    "            'last': response.css('.last::attr(href)').get()\n",
    "        }\n",
    "        \n",
    "        # 提取當前頁面的具體數據\n",
    "        page_data = self.extract_page_data(response)\n",
    "        \n",
    "        # 創建包含所有信息的字典\n",
    "        yield {\n",
    "            'url': response.url,\n",
    "            'title': title,\n",
    "            'text_content': all_text,\n",
    "            'pagination': pagination,\n",
    "            'page_data': page_data\n",
    "        }\n",
    "        \n",
    "        # 如果存在下一頁，繼續爬取\n",
    "        next_page = pagination['next']\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "\n",
    "    def extract_page_data(self, response):\n",
    "        # 這個方法需要根據您的網頁結構進行定制\n",
    "        paragraphs = response.css('p::text').getall()\n",
    "        return {\n",
    "            'paragraphs': paragraphs\n",
    "            # 可以根據需要添加更多具體的數據字段\n",
    "        }\n",
    "\n",
    "# 配置日誌\n",
    "configure_logging()\n",
    "\n",
    "# 創建一個全局變量來存儲結果\n",
    "results = []\n",
    "\n",
    "# 定義一個回調函數來收集結果\n",
    "def collect_results(item, response, spider):\n",
    "    results.append(item)\n",
    "\n",
    "# 設置信號來收集結果\n",
    "from scrapy.signalmanager import dispatcher\n",
    "dispatcher.connect(collect_results, signal=scrapy.signals.item_scraped)\n",
    "\n",
    "@wait_for(600)  # 等待最多180秒\n",
    "def run_spider():\n",
    "    \"\"\"\n",
    "    運行爬蟲並返回結果\n",
    "    \"\"\"\n",
    "    runner = CrawlerRunner(get_project_settings())\n",
    "    deferred = runner.crawl(EnhancedSpider)\n",
    "    deferred.addCallback(lambda _: reactor.stop())\n",
    "    reactor.run()\n",
    "    return deferred\n",
    "\n",
    "# 使用ThreadPoolExecutor來運行爬蟲\n",
    "def execute_spider():\n",
    "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        future = executor.submit(run_spider)\n",
    "        return future.result()\n",
    "\n",
    "# 運行爬蟲\n",
    "execute_spider()\n",
    "\n",
    "# 將結果保存為 JSON 文件\n",
    "with open('enhanced_output.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"爬蟲完成，結果已保存到 enhanced_output.json 文件中\")\n",
    "\n",
    "# 顯示前幾個結果的部分內容\n",
    "# print(\"\\n前3個結果的摘要:\")\n",
    "# for item in results[:3]:\n",
    "#     print(f\"URL: {item['url']}\")\n",
    "#     print(f\"Title: {item['title']}\")\n",
    "#     print(f\"Pagination: {item['pagination']}\")\n",
    "#     print(f\"Text content (first 100 characters): {item['text_content'][:100]}...\")\n",
    "#     print(f\"Page data: {item['page_data']}\")\n",
    "#     print(\"---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
