{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcdebf97952147178e5aae73493cf3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:558: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "華星光 => person\n",
      "三晃 => person\n",
      "永光則於本期封面故事的個股中 => person\n"
     ]
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_medium-v2.1\")\n",
    "\n",
    "# text = \"\"\"\n",
    "# Cristiano Ronaldo dos Santos Aveiro (Portuguese pronunciation: [kɾiʃˈtjɐnu ʁɔˈnaldu]; born 5 February 1985) is a Portuguese professional footballer who plays as a forward for and captains both Saudi Pro League club Al Nassr and the Portugal national team. Widely regarded as one of the greatest players of all time, Ronaldo has won five Ballon d'Or awards,[note 3] a record three UEFA Men's Player of the Year Awards, and four European Golden Shoes, the most by a European player. He has won 33 trophies in his career, including seven league titles, five UEFA Champions Leagues, the UEFA European Championship and the UEFA Nations League. Ronaldo holds the records for most appearances (183), goals (140) and assists (42) in the Champions League, goals in the European Championship (14), international goals (128) and international appearances (205). He is one of the few players to have made over 1,200 professional career appearances, the most by an outfield player, and has scored over 850 official senior career goals for club and country, making him the top goalscorer of all time.\n",
    "# \"\"\"\n",
    "\n",
    "text = \"\"\"\n",
    "CoWoS與矽光子指標 外溢特化股，目前反倒是股價低和基期低，且有科技題材性與業績搭配的新概念股族群，成為資金關切的焦點股，如台積電(2330)先進封裝CoWoS因擴充產能所帶動的設備供應鏈與矽光子族群，像萬潤(6187)、辛耘(3583)、華星光(4979) 及聯鈞(3450)等，都是受惠於台積電先封裝產能擴充進而帶動個股營收成長及股價屢創新高的個股。另有一檔前鼎(4908)與面板股友輝(4933)，於本期封面故事後個股中，有精闢的分析報導。除此，先進封裝CoWoS的產能擴充效應還外溢到化工股，尤其是特用化學做為封裝用的阻光劑及蝕刻液等，包括永光(1711)、三晃(1721)近日來都是化工股中表現量能強勢的個股，其中三晃在連漲7日後，24日開盤則呈現漲多壓回整理的格局，終場股價下跌0.3元、收在25.4元。永光則於本期封面故事的個股中，有更詳細的介紹，值得一看。面板級封裝(FOPLP)部份，日月光控股(3711)與鑫科(3663)二檔近日於台股與櫃買市場上量能表現不俗，其中日月光股價在連2漲後，於24日熄火收在平盤157元；鑫科則是連3漲後拉回修正24日股價下跌1.6元、收在95元，成交量分別為1.98萬張及6千餘張，顯示市場對面板級封裝題材的個股熱度未退。\n",
    "\"\"\"\n",
    "\n",
    "labels = [\"company enterprise\", \"person\",  \"stock number\", \"four integer numbers\", \"concept\"]\n",
    "# 定義要識別的實體標籤\n",
    "# labels = [\"stock\", \"族群\", \"行情\", \"題材\"]\n",
    "\n",
    "entities = model.predict_entities(text, labels)\n",
    "\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
