{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "import time\n",
    "import html2text\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def get_page_content(url, retries=5, delay=4):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "            }\n",
    "            response = requests.get(url, timeout=10, headers= headers)\n",
    "            response.raise_for_status()  # 檢查請求是否成功\n",
    "            response.encoding = response.apparent_encoding  # 自動檢測編碼\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"Retrying {url} in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "    return None\n",
    "\n",
    "#如果網站有強大的反爬蟲機制，你可以使用 Selenium 來模擬真實的瀏覽器行為。\n",
    "def get_page_selenium(url, retries=5, delay=4):\n",
    "    # 設置瀏覽器驅動\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    # 等待具有特定class属性的div标签出现并点击\n",
    "    # wait = WebDriverWait(driver, 5)\n",
    "    # div_element = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"button#moreBtn\")))\n",
    "    # div_element.click()\n",
    "    # div_element = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"button#moreBtn\")))\n",
    "    # div_element.click()\n",
    "\n",
    "    # 獲取頁面內容\n",
    "    page_source = driver.page_source\n",
    "    # print(page_source)\n",
    "    # 關閉瀏覽器\n",
    "    driver.quit()\n",
    "    return page_source\n",
    "\n",
    "def is_same_domain(url1, url2):\n",
    "    return urlparse(url1).netloc == urlparse(url2).netloc\n",
    "\n",
    "def is_next_level(base_url, href):\n",
    "    base_path = urlparse(base_url).path\n",
    "    href_path = urlparse(href).path\n",
    "    return href_path.startswith(base_path) and href_path != base_path\n",
    "\n",
    "def is_under_specific_url(base_url, href, specific_url):\n",
    "    specific_path = urlparse(specific_url).path\n",
    "    href_path = urlparse(href).path\n",
    "    return href_path.startswith(specific_path)\n",
    "\n",
    "def find_all_links(soup, base_url, specific_url):\n",
    "    links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        full_url = urljoin(base_url, href)\n",
    "        if (is_same_domain(base_url, full_url) and\n",
    "            is_next_level(base_url, full_url) and\n",
    "            is_under_specific_url(base_url, full_url, specific_url)):\n",
    "            links.append(full_url)\n",
    "    return links\n",
    "\n",
    "#使用一個字典來存儲標籤和相應的不想要的 class 列表\n",
    "def remove_elements_by_class(soup, tag, unwanted_classes):\n",
    "    for class_name in unwanted_classes:\n",
    "        for element in soup.find_all(tag, class_=class_name):\n",
    "            element.decompose()\n",
    "\n",
    "def keep_elements_by_class(soup, tag, wanted_classes):\n",
    "    # 找到並保留想要的元素\n",
    "    wanted_elements = []\n",
    "    for class_name in wanted_classes:\n",
    "        for element in soup.find_all(tag, class_=class_name):\n",
    "            wanted_elements.append(element.extract())\n",
    "\n",
    "    # 移除其他不想要的元素\n",
    "    for element in soup.find_all(tag):\n",
    "        element.decompose()\n",
    "\n",
    "    # 將保留的元素重新插入到原始位置\n",
    "    for element in wanted_elements:\n",
    "        soup.append(element)\n",
    "\n",
    "def extractor(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # 移除腳本和樣式標籤\n",
    "    unwanted_tags = ['script','link','style','meta','noscript','head','header','footer','select','label','legend','form','img','button','table','figcaption']\n",
    "    for script_or_style in soup(unwanted_tags):\n",
    "        script_or_style.decompose()\n",
    "\n",
    "    unwanted_classes =  ['copied']\n",
    "    for class_name in unwanted_classes:\n",
    "        for div in soup.find_all('div', id=class_name):\n",
    "            div.decompose()\n",
    "\n",
    "    #定義要移除的 class 列表\n",
    "    #unwanted_classes = ['hide-component', 'loading', 'unwanted-class3']\n",
    "    unwanted_classes = {\n",
    "        'div': ['bread', 'cookie-concent', 'gotop', 'menuBtn', 'darkLoad', 'market-widget',\n",
    "                'scroll-box', 'hotnews', 'important-info', 'nav-main__sub-menu', 'content__header',\n",
    "                'livenews__switch', 'article-function','fbmsg-box','box__promo',\n",
    "                'list-box relatednews', 'relatednews', 'ad-recommend', 'ad-box','theme-switch'],\n",
    "        'section': ['l-rating'],\n",
    "        'p': ['hint','box-title'],\n",
    "        'a': ['goMain', 'header__main-logo-square', 'nav-main__collapse-btn'],\n",
    "        'ul': ['bread-crumb'],\n",
    "        'li': ['nav-main__menu-item']\n",
    "    }\n",
    "    # # 移除不想要的元素\n",
    "    for tag, classes in unwanted_classes.items():\n",
    "        remove_elements_by_class(soup, tag, classes)\n",
    "\n",
    "    # # 定義要保留的 class 列表\n",
    "    # wanted_classes = {\n",
    "    #     'div': ['newslist livenews'],\n",
    "    # }\n",
    "\n",
    "    # # 保留想要的元素並移除其他不想要的元素\n",
    "    # for tag, classes in wanted_classes.items():\n",
    "    #     keep_elements_by_class(soup, tag, classes)\n",
    "\n",
    "    # 提取文字內容\n",
    "    text = '' #soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "    # 將 HTML 轉換為 Markdown\n",
    "    h = html2text.HTML2Text()\n",
    "    h.ignore_links = True  # 保留鏈接\n",
    "    h.ignore_images = True\n",
    "    h.ignore_mailto_links = True\n",
    "    # h.wrap_list_items = True\n",
    "    # h.wrap_links = True\n",
    "    # h.wrap_tables = True\n",
    "    markdown_text = h.handle(str(soup))\n",
    "\n",
    "    return soup, text, markdown_text\n",
    "\n",
    "\n",
    "def crawl_page(i_req, parent_url, url, depth=0, max_depth=2, result=None):\n",
    "    if result is None:\n",
    "        result = []\n",
    "\n",
    "    if depth > max_depth:\n",
    "        return result\n",
    "\n",
    "    html_content = get_page_selenium(url)\n",
    "    if not html_content:\n",
    "        return result\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # 提取 <title> 標籤的內容\n",
    "    title_tag = soup.find('title')\n",
    "    title_text = title_tag.get_text(strip=True) if title_tag else ''\n",
    "\n",
    "\n",
    "    # 找到 <meta> 標籤中 name 屬性為 \"description\" 的標籤，並提取內容出來\n",
    "    meta_description = soup.find('meta', attrs={'name': 'description'})\n",
    "    meta_description_text = ''\n",
    "    meta_description_text = meta_description['content'] if meta_description and 'content' in meta_description.attrs else ''\n",
    "\n",
    "    soup2, extract_text, markdown_text = extractor(html_content)\n",
    "    # print(soup2)\n",
    "    \n",
    "    base_url = 'https://www.ctee.com.tw/news'\n",
    "    specific_url = 'https://www.ctee.com.tw/news'\n",
    "    all_links = find_all_links(soup2, base_url, specific_url)  # 這邊原來是soup全部的內容\n",
    "    print('all-links',all_links)\n",
    "\n",
    "    i_req += 1\n",
    "\n",
    "    print(i_req, url)\n",
    "    page_data = {\n",
    "        \"i_req\": i_req,\n",
    "        \"url\": url,\n",
    "        \"parent_url\": parent_url,\n",
    "        'title': title_text,\n",
    "        'meta_description': meta_description_text,\n",
    "        'md': markdown_text,\n",
    "    }\n",
    "\n",
    "    result.append(page_data)\n",
    "\n",
    "    # 立即將結果寫入JSON文件\n",
    "    outfile = 'Result/玉山銀行/ESun.20241023.json'\n",
    "    with open(outfile, 'w', encoding='utf-8') as file:\n",
    "        json.dump(result, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    for link in all_links:\n",
    "        i_req += 1\n",
    "        result = crawl_page(i_req, url, link, depth + 1, max_depth, result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def main(url, max_depth=4):\n",
    "    i_req = 1\n",
    "    result = crawl_page(i_req=i_req, parent_url='root', url= url, max_depth=max_depth)\n",
    "    if result:\n",
    "        print(f\"Content saved to page_content.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # url = \"https://www.esunbank.com/zh-tw/personal/credit-card/discount/shops\"  # 替換為你要抓取的網頁URL\n",
    "    # url = \"https://www.esunbank.com/zh-tw/about/faq/content?q=credit_card/001\"\n",
    "    # url = \"https://www.esunbank.com/zh-tw/personal/credit-card/discount/shops/all?category=onlineshop\"  #優惠總覽 - 線上購物\n",
    "    # url = \"https://www.esunbank.com/zh-tw/about/faq/content?q=credit_card/001\"  #如何查詢信用卡辦卡進度?     \n",
    "    # url = \"https://www.fsc.gov.tw/ch/home.jsp?id=2&parentpath=0\" \n",
    "    # url = \"https://www.ctee.com.tw/livenews/stock\"\n",
    "    url = \"https://www.esunbank.com/zh-tw/personal/credit-card/tools\"\n",
    "    # url = \"https://www.ctee.com.tw/stock/star\"\n",
    "    main(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 從實體的 JSON 檔案讀取資料\n",
    "input_file = 'Result/玉山銀行/ESun-2.20241023.a.json'  # 請將 'input.json' 替換為你的 JSON 檔案名稱\n",
    "output_file = 'Result/玉山銀行/ESun-2.20241023.b.json'  # 輸出的 JSON 檔案名稱\n",
    "duplicate_file = 'Result/玉山銀行/ESun-2.20241023.duplicates.json'  # 重複資料的 JSON 檔案名稱\n",
    "\n",
    "# 讀取 JSON 檔案\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 使用一個集合來追蹤已經見過的 URL\n",
    "seen_urls = set()\n",
    "unique_data = []\n",
    "duplicate_data = []\n",
    "\n",
    "for item in data:\n",
    "    if item['url'] not in seen_urls:\n",
    "        seen_urls.add(item['url'])\n",
    "        unique_data.append(item)\n",
    "    else:\n",
    "        duplicate_data.append(item)\n",
    "\n",
    "# 將唯一的結果轉換回 JSON 格式\n",
    "unique_json_data = json.dumps(unique_data, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 將唯一的結果寫入 output_file 檔案\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(unique_json_data)\n",
    "\n",
    "# 將重複的結果轉換回 JSON 格式\n",
    "duplicate_json_data = json.dumps(duplicate_data, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 將重複的結果寫入 duplicate_file 檔案\n",
    "with open(duplicate_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(duplicate_json_data)\n",
    "\n",
    "print(f\"唯一的資料已寫入 {output_file} 檔案。\")\n",
    "print(f\"重複的資料已寫入 {duplicate_file} 檔案。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 整理出已經爬過的網站，能夠避免重複爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已將排序後的 URL 寫入 Result/玉山銀行/visited_url.txt 檔案。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 讀取 JSON 檔案\n",
    "# input_file =  'Result/玉山銀行/visited_url.txt'  # 請將 'input.json' 替換為你的 JSON 檔案名稱\n",
    "output_file = 'Result/玉山銀行/visited_url.txt'  # 輸出的文字檔案名稱\n",
    "\n",
    "# # 讀取 JSON 檔案\n",
    "# with open(input_file, 'r', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# 使用一個集合來追蹤已經見過的 URL\n",
    "visited_urls = set()\n",
    "\n",
    "# 讀取現有的 visited_url.txt 檔案內容\n",
    "try:\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        existing_urls = f.read().split('\\n')\n",
    "        visited_urls.update(existing_urls)\n",
    "except FileNotFoundError:\n",
    "    pass  # 如果檔案不存在，則跳過讀取步驟\n",
    "\n",
    "# # 抽取 url 和 parent_url，並排除重複的 URL\n",
    "# for item in data:\n",
    "#     visited_urls.add(item['url'])\n",
    "#     visited_urls.add(item['parent_url'])\n",
    "\n",
    "# 將 URL 排序\n",
    "sorted_urls = sorted(visited_urls)\n",
    "\n",
    "# 將排序後的 URL 寫入文字檔案，以逗號區隔開\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(sorted_urls))\n",
    "\n",
    "print(f\"已將排序後的 URL 寫入 {output_file} 檔案。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
