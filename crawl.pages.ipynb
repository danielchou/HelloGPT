{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "import time\n",
    "import html2text\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def get_page_content(url, retries=5, delay=4):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "            }\n",
    "            response = requests.get(url, timeout=10, headers= headers)\n",
    "            response.raise_for_status()  # 檢查請求是否成功\n",
    "            response.encoding = response.apparent_encoding  # 自動檢測編碼\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"Retrying {url} in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "    return None\n",
    "\n",
    "#如果網站有強大的反爬蟲機制，你可以使用 Selenium 來模擬真實的瀏覽器行為。\n",
    "def get_page_selenium(url, retries=5, delay=4):\n",
    "    # 設置瀏覽器驅動\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    # 等待具有特定class属性的div标签出现并点击\n",
    "    # wait = WebDriverWait(driver, 5)\n",
    "    # div_element = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"button#moreBtn\")))\n",
    "    # div_element.click()\n",
    "    # div_element = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"button#moreBtn\")))\n",
    "    # div_element.click()\n",
    "\n",
    "    # 獲取頁面內容\n",
    "    page_source = driver.page_source\n",
    "    # print(page_source)\n",
    "    # 關閉瀏覽器\n",
    "    driver.quit()\n",
    "    return page_source\n",
    "\n",
    "def is_same_domain(url1, url2):\n",
    "    return urlparse(url1).netloc == urlparse(url2).netloc\n",
    "\n",
    "def is_next_level(base_url, href):\n",
    "    base_path = urlparse(base_url).path\n",
    "    href_path = urlparse(href).path\n",
    "    return href_path.startswith(base_path) and href_path != base_path\n",
    "\n",
    "def is_under_specific_url(base_url, href, specific_url):\n",
    "    specific_path = urlparse(specific_url).path\n",
    "    href_path = urlparse(href).path\n",
    "    return href_path.startswith(specific_path)\n",
    "\n",
    "def find_all_links(soup, base_url, specific_url):\n",
    "    links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        full_url = urljoin(base_url, href)\n",
    "        if (is_same_domain(base_url, full_url) and\n",
    "            is_next_level(base_url, full_url) and\n",
    "            is_under_specific_url(base_url, full_url, specific_url)):\n",
    "            links.append(full_url)\n",
    "    return links\n",
    "\n",
    "#使用一個字典來存儲標籤和相應的不想要的 class 列表\n",
    "def remove_elements_by_class(soup, tag, unwanted_classes):\n",
    "    for class_name in unwanted_classes:\n",
    "        for element in soup.find_all(tag, class_=class_name):\n",
    "            element.decompose()\n",
    "\n",
    "def keep_elements_by_class(soup, tag, wanted_classes):\n",
    "    # 找到並保留想要的元素\n",
    "    wanted_elements = []\n",
    "    for class_name in wanted_classes:\n",
    "        for element in soup.find_all(tag, class_=class_name):\n",
    "            wanted_elements.append(element.extract())\n",
    "\n",
    "    # 移除其他不想要的元素\n",
    "    for element in soup.find_all(tag):\n",
    "        element.decompose()\n",
    "\n",
    "    # 將保留的元素重新插入到原始位置\n",
    "    for element in wanted_elements:\n",
    "        soup.append(element)\n",
    "\n",
    "def extractor(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # 移除腳本和樣式標籤\n",
    "    unwanted_tags = ['script','link','style','meta','noscript','head','header','footer','select','label','legend','form','img','button','table','figcaption']\n",
    "    for script_or_style in soup(unwanted_tags):\n",
    "        script_or_style.decompose()\n",
    "\n",
    "    unwanted_classes =  ['copied']\n",
    "    for class_name in unwanted_classes:\n",
    "        for div in soup.find_all('div', id=class_name):\n",
    "            div.decompose()\n",
    "\n",
    "    #定義要移除的 class 列表\n",
    "    #unwanted_classes = ['hide-component', 'loading', 'unwanted-class3']\n",
    "    unwanted_classes = {\n",
    "        'div': ['bread', 'cookie-concent', 'gotop', 'menuBtn', 'darkLoad', 'market-widget',\n",
    "                'scroll-box', 'hotnews', 'important-info', 'nav-main__sub-menu', 'content__header',\n",
    "                'livenews__switch', 'article-function','fbmsg-box','box__promo',\n",
    "                'list-box relatednews', 'relatednews', 'ad-recommend', 'ad-box','theme-switch'],\n",
    "        'section': ['l-rating'],\n",
    "        'p': ['hint','box-title'],\n",
    "        'a': ['goMain', 'header__main-logo-square', 'nav-main__collapse-btn'],\n",
    "        'ul': ['bread-crumb'],\n",
    "        'li': ['nav-main__menu-item']\n",
    "    }\n",
    "    # # 移除不想要的元素\n",
    "    for tag, classes in unwanted_classes.items():\n",
    "        remove_elements_by_class(soup, tag, classes)\n",
    "\n",
    "    # # 定義要保留的 class 列表\n",
    "    # wanted_classes = {\n",
    "    #     'div': ['newslist livenews'],\n",
    "    # }\n",
    "\n",
    "    # # 保留想要的元素並移除其他不想要的元素\n",
    "    # for tag, classes in wanted_classes.items():\n",
    "    #     keep_elements_by_class(soup, tag, classes)\n",
    "\n",
    "    # 提取文字內容\n",
    "    text = '' #soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "    # 將 HTML 轉換為 Markdown\n",
    "    h = html2text.HTML2Text()\n",
    "    h.ignore_links = True  # 保留鏈接\n",
    "    h.ignore_images = True\n",
    "    h.ignore_mailto_links = True\n",
    "    # h.wrap_list_items = True\n",
    "    # h.wrap_links = True\n",
    "    # h.wrap_tables = True\n",
    "    markdown_text = h.handle(str(soup))\n",
    "\n",
    "    return soup, text, markdown_text\n",
    "\n",
    "\n",
    "def crawl_page(i_req, parent_url, url, depth=0, max_depth=2, result=None):\n",
    "    if result is None:\n",
    "        result = []\n",
    "\n",
    "    if depth > max_depth:\n",
    "        return result\n",
    "\n",
    "    html_content = get_page_selenium(url)\n",
    "    if not html_content:\n",
    "        return result\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # 提取 <title> 標籤的內容\n",
    "    title_tag = soup.find('title')\n",
    "    title_text = title_tag.get_text(strip=True) if title_tag else ''\n",
    "\n",
    "\n",
    "    # 找到 <meta> 標籤中 name 屬性為 \"description\" 的標籤，並提取內容出來\n",
    "    meta_description = soup.find('meta', attrs={'name': 'description'})\n",
    "    meta_description_text = ''\n",
    "    meta_description_text = meta_description['content'] if meta_description and 'content' in meta_description.attrs else ''\n",
    "\n",
    "    soup2, extract_text, markdown_text = extractor(html_content)\n",
    "    # print(soup2)\n",
    "    \n",
    "    base_url = 'https://www.ctee.com.tw/news'\n",
    "    specific_url = 'https://www.ctee.com.tw/news'\n",
    "    all_links = find_all_links(soup2, base_url, specific_url)  # 這邊原來是soup全部的內容\n",
    "    print('all-links',all_links)\n",
    "\n",
    "    i_req += 1\n",
    "\n",
    "    print(i_req, url)\n",
    "    page_data = {\n",
    "        \"i_req\": i_req,\n",
    "        \"url\": url,\n",
    "        \"parent_url\": parent_url,\n",
    "        'title': title_text,\n",
    "        'meta_description': meta_description_text,\n",
    "        'md': markdown_text,\n",
    "    }\n",
    "\n",
    "    result.append(page_data)\n",
    "\n",
    "    # 立即將結果寫入JSON文件\n",
    "    outfile = 'Result/玉山銀行/ESun.20241023.json'\n",
    "    with open(outfile, 'w', encoding='utf-8') as file:\n",
    "        json.dump(result, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    for link in all_links:\n",
    "        i_req += 1\n",
    "        result = crawl_page(i_req, url, link, depth + 1, max_depth, result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def main(url, max_depth=4):\n",
    "    i_req = 1\n",
    "    result = crawl_page(i_req=i_req, parent_url='root', url= url, max_depth=max_depth)\n",
    "    if result:\n",
    "        print(f\"Content saved to page_content.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # url = \"https://www.esunbank.com/zh-tw/personal/credit-card/discount/shops\"  # 替換為你要抓取的網頁URL\n",
    "    # url = \"https://www.esunbank.com/zh-tw/about/faq/content?q=credit_card/001\"\n",
    "    # url = \"https://www.esunbank.com/zh-tw/personal/credit-card/discount/shops/all?category=onlineshop\"  #優惠總覽 - 線上購物\n",
    "    # url = \"https://www.esunbank.com/zh-tw/about/faq/content?q=credit_card/001\"  #如何查詢信用卡辦卡進度?     \n",
    "    # url = \"https://www.fsc.gov.tw/ch/home.jsp?id=2&parentpath=0\" \n",
    "    # url = \"https://www.ctee.com.tw/livenews/stock\"\n",
    "    url = \"https://www.esunbank.com/zh-tw/personal/credit-card/tools\"\n",
    "    # url = \"https://www.ctee.com.tw/stock/star\"\n",
    "    main(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 從實體的 JSON 檔案讀取資料\n",
    "input_file = 'Result/玉山銀行/ESun-2.20241023.a.json'  # 請將 'input.json' 替換為你的 JSON 檔案名稱\n",
    "output_file = 'Result/玉山銀行/ESun-2.20241023.b.json'  # 輸出的 JSON 檔案名稱\n",
    "duplicate_file = 'Result/玉山銀行/ESun-2.20241023.duplicates.json'  # 重複資料的 JSON 檔案名稱\n",
    "\n",
    "# 讀取 JSON 檔案\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 使用一個集合來追蹤已經見過的 URL\n",
    "seen_urls = set()\n",
    "unique_data = []\n",
    "duplicate_data = []\n",
    "\n",
    "for item in data:\n",
    "    if item['url'] not in seen_urls:\n",
    "        seen_urls.add(item['url'])\n",
    "        unique_data.append(item)\n",
    "    else:\n",
    "        duplicate_data.append(item)\n",
    "\n",
    "# 將唯一的結果轉換回 JSON 格式\n",
    "unique_json_data = json.dumps(unique_data, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 將唯一的結果寫入 output_file 檔案\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(unique_json_data)\n",
    "\n",
    "# 將重複的結果轉換回 JSON 格式\n",
    "duplicate_json_data = json.dumps(duplicate_data, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 將重複的結果寫入 duplicate_file 檔案\n",
    "with open(duplicate_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(duplicate_json_data)\n",
    "\n",
    "print(f\"唯一的資料已寫入 {output_file} 檔案。\")\n",
    "print(f\"重複的資料已寫入 {duplicate_file} 檔案。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 整理出已經爬過的網站，能夠避免重複爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已將排序後的 URL 寫入 Result/玉山銀行/visited_url.txt 檔案。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 讀取 JSON 檔案\n",
    "# input_file =  'Result/玉山銀行/visited_url.txt'  # 請將 'input.json' 替換為你的 JSON 檔案名稱\n",
    "output_file = 'Result/玉山銀行/visited_url.txt'  # 輸出的文字檔案名稱\n",
    "\n",
    "# # 讀取 JSON 檔案\n",
    "# with open(input_file, 'r', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# 使用一個集合來追蹤已經見過的 URL\n",
    "visited_urls = set()\n",
    "\n",
    "# 讀取現有的 visited_url.txt 檔案內容\n",
    "try:\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        existing_urls = f.read().split('\\n')\n",
    "        visited_urls.update(existing_urls)\n",
    "except FileNotFoundError:\n",
    "    pass  # 如果檔案不存在，則跳過讀取步驟\n",
    "\n",
    "# # 抽取 url 和 parent_url，並排除重複的 URL\n",
    "# for item in data:\n",
    "#     visited_urls.add(item['url'])\n",
    "#     visited_urls.add(item['parent_url'])\n",
    "\n",
    "# 將 URL 排序\n",
    "sorted_urls = sorted(visited_urls)\n",
    "\n",
    "# 將排序後的 URL 寫入文字檔案，以逗號區隔開\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(sorted_urls))\n",
    "\n",
    "print(f\"已將排序後的 URL 寫入 {output_file} 檔案。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from mistralai import Mistral\n",
    "\n",
    "load_dotenv() # 加載 .env 文件\n",
    "\n",
    "api_key = os.getenv('MISTRAL_API_KEY')\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "# 初始化Mistral客戶端\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "# 你要總結的文章內容\n",
    "article_content = \"\"\"\n",
    "受到台電土建工程因缺工而進度落後的拖累，整體重電股9月營收月增率不如預期，其中，中興電(1513) 9月營收月減4.1%表現最差，士電(1503)、亞力(1514)則因季底集中出貨，9月營收月增15%~16%表現最佳，受此激勵，士電今(14)日早盤股價一度上漲近5%，亞力也有約4%漲幅，華城(1519)營收月增逾11%，早盤股價漲幅逾3%。\n",
    "\n",
    "法人表示，重電股近二年營運成長動能無虞，但短線受到美國總統選舉的關係，美國客戶擔心之前基建法案、降低通膨法案的補貼可能不會再追加，因此下單有暫停觀望的現象，短線較不利重電類股的股價表現。\n",
    "\n",
    "受到台電土建工程因缺工而進度落後的拖累，開關設備大廠中興電9月合併營收21.05億元，月減4.1%表現最差，主要是設備出貨被遞延的情況較為嚴重，但仍年成長23.15％，士電、亞力則因季底集中出貨，9月營收月增15%~16%表現最佳，華城9月營收17.66億元，月增11.25%、年增22.51%。受惠台電強韌電網計畫助陣，機電四雄士電、中興電、亞力及華城前三季營收均飆同期新高。\n",
    "\n",
    "展望未來，法人表示，受惠台電強韌電網計畫、全球能源轉型政策、外銷市場、AI資料中心持續建置對電力設備需求維持成長，目前重電類股的訂單金額均位於歷史高檔區，能見度至少1-2 年，加上產能持續擴充，營運成長動能無虞。不過，近期受到美國總統選舉的關係，美國客戶擔心之前基建法案、降低通膨法案的補貼可能不會再追加，因此下單有暫停觀望的現象，拖累重電類股的股價表現，建議靜待政治風險消除後，再行進場抄底。\n",
    "\n",
    "另有法人認為，重電一哥華城雖短期營運因交期影響低於預期，但整體仍受惠全球淨零轉型需求，預期今年第四季迎來工程營運旺季，加上華城仍是全台唯一生產500kv變壓器的廠商，國內產業具有一定地位，維持華城「買進」的投資建議。\"\"\"\n",
    "\n",
    "# 發送請求進行總結\n",
    "# chat_response = client.chat.complete(\n",
    "#     model=model,\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": f\"今天你是一個優秀的台股分析師，請以下面這篇文章\\n\\n{article_content}\\n\\n幫我整理分析出我要的JSON格式，條件如下： \\\n",
    "# 1.請分析找出市場商機與對應的概念股票。 \\\n",
    "# 2.請以市場商機是key,對應的概念股是value 為資料格式。 \\\n",
    "# 3.[市場商機]當中的相關概念股內有重複的公司股票名稱，該市場商機的項目就不要列出來。 \\\n",
    "# 4.[市場商機]名稱很相似的也請別重複列出來，挑選一個最好的，且不要有'市場'的字眼，且不要超過八個字。 \\\n",
    "# 請好好的推論，回傳出來的JSON內的value請保持絕對唯一，不要在JSON當中有重複的狀況。但最後請只要給我JSON格式就好。\"\n",
    "#         },\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# 發送請求進行總結\n",
    "chat_response = client.chat.complete(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"今天你是一個優秀的台股分析師，請以下面這篇文章\\n\\n{article_content}\\n\\n幫我整理分析出我要的JSON格式，條件如下： \\\n",
    "1.請分析找出市場商機與對應的概念股票。 \\\n",
    "2.請以股票是key,其股票的題材關鍵字是value，題材關鍵字請別超過五個，每個也請不要超過8個字。 \\\n",
    "請好好的推論，回傳出來的JSON內的value請保持絕對唯一，不要在JSON當中有重複的狀況。但最後請只要給我JSON格式就好。 \\\n",
    "JSON之後，請再加上總結出到底這些股票看空還是看多，理由是什麼? 理由請別超過30個字。\"\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "# 打印模型的回應\n",
    "print(chat_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
